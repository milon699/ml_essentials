{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - Lucas Oswald, Maximilian Stucke, Milon Miah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from abc import abstractmethod\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "      this class will later get the following attributes\n",
    "      all nodes:\n",
    "          features\n",
    "          responses\n",
    "      split nodes additionally:\n",
    "          left\n",
    "          right\n",
    "          split_index\n",
    "          threshold\n",
    "      leaf nodes additionally\n",
    "          prediction\n",
    "    '''\n",
    "        \n",
    "\n",
    "class Tree:\n",
    "    '''\n",
    "      base class for RegressionTree and ClassificationTree\n",
    "    '''\n",
    "    def __init__(self, n_min=10):\n",
    "        '''n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        self.n_min = n_min \n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' return the prediction for the given 1-D feature vector x\n",
    "        '''\n",
    "        # first find the leaf containing the 1-D feature vector x\n",
    "        node = self.root\n",
    "        while not hasattr(node, \"prediction\"):\n",
    "            j = node.split_index\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        # finally, return the leaf's prediction\n",
    "        return node.prediction\n",
    "        \n",
    "    def train(self, features, responses, D_try=None):\n",
    "        '''\n",
    "        features: the feature matrix of the training set\n",
    "        response: the vector of responses\n",
    "        '''\n",
    "        N, D = features.shape\n",
    "        assert(responses.shape[0] == N)\n",
    "\n",
    "        if D_try is None:\n",
    "            D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "        \n",
    "        # initialize the root node\n",
    "        self.root = Node()\n",
    "        #Only here the attributes of nodes are defined!\n",
    "        self.root.features  = features\n",
    "        self.root.responses = responses\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            active_indices = self.select_active_indices(D, D_try)\n",
    "            left, right = self.make_split_node(node, active_indices)\n",
    "            if left is None: # no split found\n",
    "                self.make_leaf_node(node)\n",
    "            else:\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "    \n",
    "    def make_split_node(self, node, indices):\n",
    "        '''\n",
    "        node: the node to be split\n",
    "        indices: a numpy array of length 'D_try', containing the feature \n",
    "                         indices to be considered for the present split\n",
    "                         \n",
    "        return: None, None -- if no suitable split has been found, or\n",
    "                left, right -- the children of the split\n",
    "        '''\n",
    "        # all responses equal => no improvement possible by any split\n",
    "        if np.unique(node.responses).shape[0] == 1:\n",
    "            return None, None\n",
    "        \n",
    "        # find best feature j_min (among 'indices') and best threshold t_min for the split\n",
    "        l_min = float('inf')  # upper bound for the loss, later the loss of the best split\n",
    "        j_min, t_min = None, None\n",
    "\n",
    "        for j in indices:\n",
    "            thresholds = self.find_thresholds(node, j)\n",
    "\n",
    "            # compute loss for each threshold\n",
    "            for t in thresholds:\n",
    "                loss = self.compute_loss_for_split(node, j, t)\n",
    "\n",
    "                # remember the best split so far \n",
    "                # (the condition is never True when loss = float('inf') )\n",
    "                if loss < l_min:\n",
    "                    l_min = loss\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "        if j_min is None: # no split found\n",
    "            return None, None\n",
    "\n",
    "        # create children for the best split\n",
    "        left, right = self.make_children(node, j_min, t_min)\n",
    "\n",
    "        # turn the current 'node' into a split node\n",
    "        # (store children and split condition)\n",
    "        node.left = left\n",
    "        node.right = right\n",
    "        node.split_index = j_min\n",
    "        node.threshold = t_min\n",
    "        \n",
    "        #raise NotImplementedError(\"make_split_node(): remove this exception after adding your code above.\")\n",
    "        \n",
    "        # return the children (to be placed on the stack)\n",
    "        return left, right\n",
    "    \n",
    "    def select_active_indices(self, D, D_try):\n",
    "        ''' return a 1-D array with D_try randomly selected indices from 0...(D-1).\n",
    "        '''\n",
    "        return np.random.randint(low = 0, high = D-1, size = D_try) # your code here\n",
    "        #raise NotImplementedError(\"select_active_indices(): remove this exception after adding your code above.\")\n",
    "        \n",
    "    def find_thresholds(self, node, j):\n",
    "        ''' return: a 1-D array with all possible thresholds along feature j\n",
    "        '''\n",
    "        #Get all values for feature j\n",
    "        feature_values = node.features[:, j]\n",
    "        \n",
    "        #Sort feature values in ascending order\n",
    "        sorted_feature_values = np.sort(feature_values)\n",
    "        \n",
    "        #Find unique values to ensure distinct thresholds\n",
    "        unique_feature_values = np.unique(sorted_feature_values)\n",
    "        \n",
    "        #Calculate the midpoints between adjacent unique values as possible thresholds\n",
    "        # [:-1], all values except for the last \n",
    "        thresholds = (unique_feature_values[:-1] + unique_feature_values[1:]) / 2\n",
    "        \n",
    "        return thresholds\n",
    "        #raise NotImplementedError(\"find_thresholds(): remove this exception after adding your code above.\")\n",
    "        \n",
    "    def make_children(self, node, j, t):\n",
    "        ''' execute the split in feature j at threshold t\n",
    "        \n",
    "            return: left, right -- the children of the split, with features and responses\n",
    "                                   properly assigned according to the split\n",
    "        '''\n",
    "        left = Node()\n",
    "        right = Node()\n",
    "\n",
    "        indices_left = np.where(node.features[:,j] < t)[0]\n",
    "        indices_right = np.where(node.features[:,j] > t)[0]\n",
    "        left.features = node.features[indices_left]\n",
    "        right.features = node.features[indices_right]\n",
    "        left.responses = node.responses[indices_left]\n",
    "        right.responses = node.responses[indices_right]\n",
    "        \n",
    "        #raise NotImplementedError(\"make_children(): remove this exception after adding your code above.\")\n",
    "        \n",
    "        return left, right\n",
    "        \n",
    "    @abstractmethod\n",
    "    def make_leaf_node(self, node):\n",
    "        ''' Turn node into a leaf by computing and setting `node.prediction`\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"make_leaf_node() must be implemented in a subclass.\")\n",
    "        \n",
    "    @abstractmethod\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        ''' Return the resulting loss when the data are split along feature j at threshold t.\n",
    "            If the split is not admissible, return float('inf').\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"compute_loss_for_split() must be implemented in a subclass.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(Tree):\n",
    "    def __init__(self, n_min=10):\n",
    "        super(RegressionTree, self).__init__(n_min)\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        \n",
    "        indices_left = np.where(node.features[:,j] < t)[0]\n",
    "        n_left = indices_left.size\n",
    "        indices_right = np.where(node.features[:,j] > t)[0]\n",
    "        n_right = indices_right.size\n",
    "        \n",
    "        if n_left >= self.n_min and n_right >= self.n_min:\n",
    "            mean_left = 1/n_left*np.sum(node.responses[indices_left])\n",
    "            mean_right = 1/n_right*np.sum(node.responses[indices_right])\n",
    "            loss = np.sum((node.responses[indices_left] - mean_left)**2) + np.sum((node.responses[indices_right] - mean_right)**2)\n",
    "            return loss\n",
    "        \n",
    "        else:\n",
    "            return float('inf')\n",
    "        \n",
    "        raise NotImplementedError(\"compute_loss_for_split(): remove this exception after adding your code above.\")\n",
    "        \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a regression tree is a real number)\n",
    "        n_node = node.responses.size\n",
    "        node.prediction = 1/n_node * np.sum(node.responses) \n",
    "        #raise NotImplementedError(\"make_leaf_node(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationTree(Tree):\n",
    "    '''implement classification tree so that it can handle arbitrary many classes\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, classes, n_min=10):\n",
    "        ''' classes: a 1-D array with the permitted class labels\n",
    "            n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        super(ClassificationTree, self).__init__(n_min)\n",
    "        self.classes = classes\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        \n",
    "        # Calculate probabilities of each class \n",
    "        indices_left = np.where(node.features[:,j] < t)[0]\n",
    "        n_left = indices_left.size\n",
    "        indices_right = np.where(node.features[:,j] > t)[0]\n",
    "        n_right = indices_right.size\n",
    "        \n",
    "        if n_left >= self.n_min and n_right >= self.n_min:\n",
    "            \n",
    "            responses_left = node.responses[indices_left]\n",
    "            responses_right = node.responses[indices_right]\n",
    "            \n",
    "            p_left = np.zeros(self.classes.size)\n",
    "            p_right = np.zeros(self.classes.size)\n",
    "            for clas in range(p_left.size):\n",
    "                \n",
    "                #Use one-hot encoding for each class\n",
    "                class_one_hot_left = np.where(responses_left == self.classes[clas], 1, 0)\n",
    "                p_left[clas] = np.mean(class_one_hot_left)\n",
    "                class_one_hot_right = np.where(responses_right == self.classes[clas], 1, 0)\n",
    "                p_right[clas] = np.mean(class_one_hot_right)\n",
    "                \n",
    "            # Compute Gini loss\n",
    "            loss = n_left*(1-np.sum(p_left**2)) + n_right*(1-np.sum(p_right**2))\n",
    "            return loss\n",
    "        \n",
    "        else:\n",
    "            return float('inf')\n",
    "        #raise NotImplementedError(\"compute_loss_for_split(): remove this exception after adding your code above.\")\n",
    "        \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a classification tree is a class label)\n",
    "        #Use hard response\n",
    "        labels, label_counts = np.unique(node.responses, return_counts = True)\n",
    "        majority_idx = np.argmax(label_counts)\n",
    "        majority = labels[majority_idx]\n",
    "        node.prediction = majority\n",
    "        #raise NotImplementedError(\"make_leaf_node(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data and extract 3s and 9s\n",
    "digits = load_digits()\n",
    "print(digits.data.shape, digits.target.shape)\n",
    "\n",
    "instances = (digits.target == 3) | (digits.target == 9)\n",
    "features = digits.data[instances, :]\n",
    "labels = digits.target[instances]\n",
    "\n",
    "# for regression, we use labels +1 and -1\n",
    "responses = np.array([1 if l == 3 else -1 for l in labels])\n",
    "\n",
    "assert(features.shape[0] == labels.shape[0] == responses.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the Regression Tree prediction of the 1th fold is 29.85 %\n",
      "Error rate of the Regression Tree prediction of the 2th fold is 42.85 %\n",
      "Error rate of the Regression Tree prediction of the 3th fold is 22.12 %\n",
      "Error rate of the Regression Tree prediction of the 4th fold is 14.94 %\n",
      "Error rate of the Regression Tree prediction of the 5th fold is 31.01 %\n",
      "\n",
      "Averaged error of the Regression Tree prediction is 28.15 +/- 4.19 %\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionTree()\n",
    "# and comment on your results\n",
    "k_folds = 5\n",
    "X_train_folds = np.zeros((k_folds, 326, 64))\n",
    "X_test_folds = np.zeros((k_folds, 37, 64))\n",
    "y_train_folds = np.zeros((k_folds, 326))\n",
    "y_test_folds = np.zeros((k_folds, 37))\n",
    "\n",
    "error_test_rate = np.zeros(k_folds)\n",
    "\n",
    "predictions = np.zeros((k_folds, 37))\n",
    "\n",
    "for i in range(k_folds):\n",
    "    \n",
    "    X_train_folds[i], X_test_folds[i], y_train_folds[i], y_test_folds[i] = model_selection.train_test_split(\n",
    "        features, responses, test_size = 0.1, random_state = i)\n",
    "    \n",
    "    #Create RegressionTree instance\n",
    "    regTree = RegressionTree()\n",
    "    \n",
    "    #Train tree with data\n",
    "    regTree.train(X_train_folds[i], y_train_folds[i])\n",
    "    \n",
    "    #Predict labels of test data\n",
    "    for j in range(X_test_folds[i].shape[0]):\n",
    "        predictions[i][j] = regTree.predict(X_test_folds[i][j])\n",
    "    \n",
    "    #Compute error for each fold\n",
    "    error_test_rate[i] = np.mean(abs(predictions[i]-y_test_folds[i])) *100\n",
    "    \n",
    "    #Print errors for each fold\n",
    "    print('Error rate of the Regression Tree prediction of the {}th fold is {:.2f} %'.format(i+1, error_test_rate[i]))\n",
    "    \n",
    "#Print averaged errors for both results\n",
    "print('\\nAveraged error of the Regression Tree prediction is {:.2f} +/- {:.2f} %'.format(error_test_rate.mean(), np.sqrt(error_test_rate.var()/len(error_test_rate))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The given task of the exercise sheet is usually addressed to classification algorithms. The output of the regression tree, though, returns the likelihood for each digit to occur. The errors are calculated as the mean differences to the true labels (-1, 1) and, thus, do not vanish even though the tendency of the regression tree to predict the right outcome might be right. This is why the errors seem to be quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the Classification Tree prediction of the 1th fold is 16.22 %\n",
      "Error rate of the Classification Tree prediction of the 2th fold is 18.92 %\n",
      "Error rate of the Classification Tree prediction of the 3th fold is 10.81 %\n",
      "Error rate of the Classification Tree prediction of the 4th fold is 10.81 %\n",
      "Error rate of the Classification Tree prediction of the 5th fold is 21.62 %\n",
      "\n",
      "Averaged error of the Classification Tree prediction is 15.68 +/- 1.93 %\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using ClassificationTree(classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "k_folds = 5\n",
    "X_train_folds = np.zeros((k_folds, 326, 64))\n",
    "X_test_folds = np.zeros((k_folds, 37, 64))\n",
    "y_train_folds = np.zeros((k_folds, 326))\n",
    "y_test_folds = np.zeros((k_folds, 37))\n",
    "\n",
    "error_test_rate = np.zeros(k_folds)\n",
    "\n",
    "predictions = np.zeros((k_folds, 37))\n",
    "\n",
    "for i in range(k_folds):\n",
    "    \n",
    "    X_train_folds[i], X_test_folds[i], y_train_folds[i], y_test_folds[i] = model_selection.train_test_split(\n",
    "        features, labels, test_size = 0.1, random_state = i)\n",
    "    \n",
    "    #Create RegressionTree instance\n",
    "    claTree = ClassificationTree(classes = np.array([3,9]), n_min = 10)\n",
    "    \n",
    "    #Train tree with data\n",
    "    claTree.train(X_train_folds[i], y_train_folds[i])\n",
    "    \n",
    "    #Predict labels of test data\n",
    "    for j in range(X_test_folds[i].shape[0]):\n",
    "        predictions[i][j] = claTree.predict(X_test_folds[i][j])\n",
    "    \n",
    "    #Compute error for each fold\n",
    "    error_test_rate[i] = np.mean(predictions[i] != y_test_folds[i])*100\n",
    "    \n",
    "    #Print errors for each fold\n",
    "    print('Error rate of the Classification Tree prediction of the {}th fold is {:.2f} %'.format(i+1, error_test_rate[i]))\n",
    "    \n",
    "#Print averaged errors for both results\n",
    "print('\\nAveraged error of the Classification Tree prediction is {:.2f} +/- {:.2f} %'.format(error_test_rate.mean(), np.sqrt(error_test_rate.var()/len(error_test_rate))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the single classification tree, the errors still seem to be quite large. Over-fitting is a major problem as features of each concrete training set are weighted very strongly in leave nodes. A forest of classification trees will lower the error by strengthening the underlying statistics of the predictions and reducing over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sampling(features, responses):\n",
    "    '''return a bootstrap sample of features and responses\n",
    "    '''\n",
    "    \n",
    "    #Create random indices with replacement via numpy\n",
    "    bootstrap_idx = np.random.choice(len(responses), len(responses), replace = True)\n",
    "    \n",
    "    bootstrap_fea = features[bootstrap_idx]\n",
    "    bootstrap_res = responses[bootstrap_idx]\n",
    "    \n",
    "    return bootstrap_fea, bootstrap_res\n",
    "    \n",
    "    #raise NotImplementedError(\"bootstrap_sampling(): remove this exception after adding your code above.\")\n",
    "\n",
    "#Define own function for one-against-the-rest classifier, as some extra work is necessary to allow for balanced +/-1 output of responses\n",
    "def bootstrap_sampling_reg(features, responses):\n",
    "    '''return a bootstrap sample of features and responses\n",
    "    '''\n",
    "    \n",
    "    #For one-against-the-rest regression implement probability vector in order to balance bootstrapped array equally\n",
    "    \n",
    "    N_p1 = responses[responses == 1].size\n",
    "    N_m1 = responses[responses == -1].size\n",
    "    N = responses.size\n",
    "    assert(N_p1 + N_m1 == N)\n",
    "    \n",
    "    #Set hyperparameter to equally distributed data for training\n",
    "    p = 0.5\n",
    "    P = np.where(responses == 1, p/N_p1, p/N_m1)\n",
    "    \n",
    "    #Create random indices with replacement via numpy\n",
    "    bootstrap_idx = np.random.choice(len(responses), len(responses), replace = True, p = P)\n",
    "    \n",
    "    bootstrap_fea = features[bootstrap_idx]\n",
    "    bootstrap_res = responses[bootstrap_idx]\n",
    "    \n",
    "    return bootstrap_fea, bootstrap_res\n",
    "    \n",
    "    #raise NotImplementedError(\"bootstrap_sampling(): remove this exception after adding your code above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegressionForest():\n",
    "    def __init__(self, n_trees, n_min=10):\n",
    "        # create ensemble\n",
    "        self.trees = [RegressionTree(n_min) for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            bootstrap_features, bootstrap_responses = bootstrap_sampling_reg(features, responses)\n",
    "            tree.train(bootstrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        prediction = 0\n",
    "        for tree in self.trees:\n",
    "            prediction += tree.predict(x)\n",
    "            \n",
    "        prediction = prediction/len(self.trees)\n",
    "            \n",
    "        return prediction\n",
    "        #raise NotImplementedError(\"predict(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationForest():\n",
    "    def __init__(self, n_trees, classes, n_min=1):\n",
    "        self.trees = [ClassificationTree(classes, n_min) for i in range(n_trees)]\n",
    "        self.classes = classes\n",
    "    \n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        # Use hard classification\n",
    "        predictions = []\n",
    "        for tree in self.trees:\n",
    "            predictions.append(tree.predict(x))\n",
    "            \n",
    "        predictions = np.array(predictions)\n",
    "        labels, label_counts = np.unique(predictions, return_counts = True)\n",
    "        majority_idx = np.argmax(label_counts)\n",
    "        majority = labels[majority_idx]\n",
    "        return majority \n",
    "        #raise NotImplementedError(\"predict(): remove this exception after adding your code above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the Regression Forest prediction of the 1th fold is 39.92 %\n",
      "Error rate of the Regression Forest prediction of the 2th fold is 40.73 %\n",
      "Error rate of the Regression Forest prediction of the 3th fold is 28.99 %\n",
      "Error rate of the Regression Forest prediction of the 4th fold is 34.07 %\n",
      "Error rate of the Regression Forest prediction of the 5th fold is 32.22 %\n",
      "\n",
      "Averaged error of the Regression Forest prediction is 35.18 +/- 2.02 %\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionForest(n_trees=10)\n",
    "# and comment on your results\n",
    "k_folds = 5\n",
    "X_train_folds = np.zeros((k_folds, 326, 64))\n",
    "X_test_folds = np.zeros((k_folds, 37, 64))\n",
    "y_train_folds = np.zeros((k_folds, 326))\n",
    "y_test_folds = np.zeros((k_folds, 37))\n",
    "\n",
    "error_test_rate = np.zeros(k_folds)\n",
    "\n",
    "predictions = np.zeros((k_folds, 37))\n",
    "\n",
    "for i in range(k_folds):\n",
    "    \n",
    "    X_train_folds[i], X_test_folds[i], y_train_folds[i], y_test_folds[i] = model_selection.train_test_split(\n",
    "        features, responses, test_size = 0.1, random_state = i)\n",
    "    \n",
    "    #Create RegressionTree instance\n",
    "    regForest = RegressionForest(n_trees = 10)\n",
    "    \n",
    "    #Train tree with data\n",
    "    regForest.train(X_train_folds[i], y_train_folds[i])\n",
    "    \n",
    "    #Predict labels of test data\n",
    "    for j in range(X_test_folds[i].shape[0]):\n",
    "        predictions[i][j] = regForest.predict(X_test_folds[i][j])\n",
    "    \n",
    "    #Compute error for each fold\n",
    "    error_test_rate[i] = np.mean(abs(predictions[i]-y_test_folds[i])) *100\n",
    "    \n",
    "    #Print errors for each fold\n",
    "    print('Error rate of the Regression Forest prediction of the {}th fold is {:.2f} %'.format(i+1, error_test_rate[i]))\n",
    "    \n",
    "#Print averaged errors for both results\n",
    "print('\\nAveraged error of the Regression Forest prediction is {:.2f} +/- {:.2f} %'.format(error_test_rate.mean(), np.sqrt(error_test_rate.var()/len(error_test_rate))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The errors remain also for the regression forest quite large, however, this is still because of the method used to compute the errors. The actual likelihoods, though, are more precise when using multiple trees and overfitting is surpressed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the Classification Forest prediction of the 1th fold is 2.70 %\n",
      "Error rate of the Classification Forest prediction of the 2th fold is 8.11 %\n",
      "Error rate of the Classification Forest prediction of the 3th fold is 5.41 %\n",
      "Error rate of the Classification Forest prediction of the 4th fold is 5.41 %\n",
      "Error rate of the Classification Forest prediction of the 5th fold is 5.41 %\n",
      "\n",
      "Averaged error of the Classification Forest prediction is 5.41 +/- 0.76 %\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using DecisionForest(n_trees=10, classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "k_folds = 5\n",
    "X_train_folds = np.zeros((k_folds, 326, 64))\n",
    "X_test_folds = np.zeros((k_folds, 37, 64))\n",
    "y_train_folds = np.zeros((k_folds, 326))\n",
    "y_test_folds = np.zeros((k_folds, 37))\n",
    "\n",
    "error_test_rate = np.zeros(k_folds)\n",
    "\n",
    "predictions = np.zeros((k_folds, 37))\n",
    "\n",
    "for i in range(k_folds):\n",
    "    \n",
    "    X_train_folds[i], X_test_folds[i], y_train_folds[i], y_test_folds[i] = model_selection.train_test_split(\n",
    "        features, labels, test_size = 0.1, random_state = i)\n",
    "    \n",
    "    #Create ClassificationTree instance\n",
    "    claForest = ClassificationForest(classes = np.array([3,9]), n_trees = 10)\n",
    "    \n",
    "    #Train tree with data\n",
    "    claForest.train(X_train_folds[i], y_train_folds[i])\n",
    "    \n",
    "    #Predict labels of test data\n",
    "    for j in range(X_test_folds[i].shape[0]):\n",
    "        predictions[i][j] = claForest.predict(X_test_folds[i][j])\n",
    "    \n",
    "    #Compute error for each fold\n",
    "    error_test_rate[i] = np.mean(predictions[i] != y_test_folds[i])*100\n",
    "    \n",
    "    #Print errors for each fold\n",
    "    print('Error rate of the Classification Forest prediction of the {}th fold is {:.2f} %'.format(i+1, error_test_rate[i]))\n",
    "    \n",
    "#Print averaged errors for both results\n",
    "print('\\nAveraged error of the Classification Forest prediction is {:.2f} +/- {:.2f} %'.format(error_test_rate.mean(), np.sqrt(error_test_rate.var()/len(error_test_rate))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Classification Forest yields very good results. Using multiple trees and the underlying statistics causes the errors to significantly decrease and prevent overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate of the Classification Forest prediction (entire dataset) of the 1th fold is 6.11 %\n",
      "Error rate of the Classification Forest prediction (entire dataset) of the 2th fold is 3.89 %\n",
      "Error rate of the Classification Forest prediction (entire dataset) of the 3th fold is 6.67 %\n"
     ]
    }
   ],
   "source": [
    "# Train DecisionForest(n_trees=10, classes=np.unique(digits.target))\n",
    "# for all 10 digits simultaneously.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "features_all = digits.data\n",
    "labels_all = digits.target\n",
    "\n",
    "k_folds = 5\n",
    "X_train_folds = np.zeros((k_folds, 1617, 64))\n",
    "X_test_folds = np.zeros((k_folds, 180, 64))\n",
    "y_train_folds = np.zeros((k_folds, 1617))\n",
    "y_test_folds = np.zeros((k_folds, 180))\n",
    "\n",
    "error_test_rate = np.zeros(k_folds)\n",
    "\n",
    "predictions = np.zeros((k_folds, 180))\n",
    "\n",
    "for i in range(k_folds):\n",
    "    \n",
    "    X_train_folds[i], X_test_folds[i], y_train_folds[i], y_test_folds[i] = model_selection.train_test_split(\n",
    "        features_all, labels_all, test_size = 0.1, random_state = i)\n",
    "    \n",
    "    #Create ClassificationTree instance\n",
    "    claForest = ClassificationForest(classes = np.unique(digits.target), n_trees = 10)\n",
    "    \n",
    "    #Train tree with data\n",
    "    claForest.train(X_train_folds[i], y_train_folds[i])\n",
    "    \n",
    "    #Predict labels of test data\n",
    "    for j in range(X_test_folds[i].shape[0]):\n",
    "        predictions[i][j] = claForest.predict(X_test_folds[i][j])\n",
    "    \n",
    "    #Compute error for each fold\n",
    "    error_test_rate[i] = np.mean(predictions[i] != y_test_folds[i])*100\n",
    "    \n",
    "    #Print errors for each fold\n",
    "    print('Error rate of the Classification Forest prediction (entire dataset) of the {}th fold is {:.2f} %'.format(i+1, error_test_rate[i]))\n",
    "    \n",
    "#Print averaged errors for both results\n",
    "print('\\nAveraged error of the Classification Forest prediction (entire dataset) is {:.2f} +/- {:.2f} %'.format(error_test_rate.mean(), np.sqrt(error_test_rate.var()/len(error_test_rate))))\n",
    "\n",
    "#Plot Confusion matrix for each fold\n",
    "for i in range(k_folds):\n",
    "    \n",
    "    cm = confusion_matrix(y_test_folds[i], predictions[i])\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title('Multi-class Classification Forest Confusion Matrix, Fold {}'.format(i+1))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also for a multi-class classification problem, the results with the classification forest are very satisfying. The non-diagonal elements of the confusion matrix are significantly lower than its diagonal for each fold of the cross-validation. The error remains in the same range as in the previous task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-against-the-rest classification with RegressionForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train ten one-against-the-rest regression forests for the 10 digits.\n",
    "# Make sure that all training sets are balanced between the current digit and the rest.\n",
    "# Assign test instances to the digit with highest score, \n",
    "# or to \"unknown\" if all scores are negative.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "\n",
    "features_all = digits.data\n",
    "labels_all = digits.target\n",
    "\n",
    "k_folds = 5\n",
    "X_train_folds = np.zeros((k_folds, 1617, 64))\n",
    "X_test_folds = np.zeros((k_folds, 180, 64))\n",
    "y_train_folds = np.zeros((k_folds, 1617))\n",
    "y_test_folds = np.zeros((k_folds, 180))\n",
    "\n",
    "error_test_rate = np.zeros(k_folds)\n",
    "\n",
    "predictions = np.zeros((k_folds, 10, 180))\n",
    "\n",
    "for i in range(k_folds):\n",
    "    \n",
    "    X_train_folds[i], X_test_folds[i], y_train_folds[i], y_test_folds[i] = model_selection.train_test_split(\n",
    "        features_all, labels_all, test_size = 0.1, random_state = i)\n",
    "\n",
    "    resp_digit = np.zeros((len(np.unique(digits.target)), 1617))\n",
    "    regForests_OAR = []\n",
    "    for digit in np.unique(digits.target):\n",
    "        \n",
    "        #Convert responses with current digit to 1 and -1 otherwise\n",
    "        resp_digit[digit] = np.array([1 if l == digit else -1 for l in y_train_folds[i]])\n",
    "        \n",
    "        regForests_OAR.append(RegressionForest(n_trees = 10))\n",
    "        \n",
    "        #Train tree with data\n",
    "        regForests_OAR[digit].train(X_train_folds[i], resp_digit[digit])\n",
    "        \n",
    "        for j in range(X_test_folds[i].shape[0]):\n",
    "                predictions[i][digit][j] = regForests_OAR[digit].predict(X_test_folds[i][j])\n",
    "                \n",
    "#Find digit with highest score \n",
    "    \n",
    "# Set all negative scores to nan\n",
    "idx, maxi = np.argmax(predictions, axis = 1), np.max(predictions, axis = 1)\n",
    "predictions_final = np.where(maxi < 0, np.nan, idx)\n",
    "\n",
    "#Compute error and plot confusion matrix\n",
    "for i in range(k_folds):\n",
    "    error_test_rate[i] = np.mean(predictions_final[i] != y_test_folds[i])*100\n",
    "    \n",
    "    #Print errors for each fold\n",
    "    print('Error rate for the one-against-the-rest RegressionForest prediction (entire dataset) of the {}th fold is {:.2f} %'.format(i+1, error_test_rate[i]))\n",
    "    predictions_final[i][np.isnan(predictions_final[i])] = -1\n",
    "    cm = confusion_matrix(y_test_folds[i], predictions_final[i])\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title('One-against-the-rest classification via RegressionForest - Confusion Matrix, Fold {}'.format(i+1))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "#Print averaged errors for both results\n",
    "print('\\nAveraged error for the one-against-the-rest RegressionForest prediction (entire dataset) is {:.2f} +/- {:.2f} %'.format(error_test_rate.mean(), np.sqrt(error_test_rate.var()/len(error_test_rate))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-against-the-rest classification with RegressionForest also delivers more or less equally reliable results. For each fold the confusion matrix is almost diagonal (with a few less significant exceptions). Considering the complexity of the code though, it was more straight-forward to use ClassificationForest and its corresponding loss function instead of computing the results for each digit individually and putting the results together at the end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
