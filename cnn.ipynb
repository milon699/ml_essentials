{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb97a8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 3.95e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███▊                                                                            | 1/21 [03:05<1:01:52, 185.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss:  2.67e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████▌                                                                  | 4/21 [12:39<53:45, 189.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15960\\2069724320.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[1;31m# compute the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "#Necessary for me to not end up in Runtime Error, if someone can give me feedback on why that is, much appreciated!\n",
    "if __name__ == '__main__':\n",
    "    #mp.set_start_method('spawn')\n",
    "    \n",
    "    plt.rc(\"figure\", dpi=100)\n",
    "    \n",
    "    batch_size = 100\n",
    "    \n",
    "    # transform images into normalized tensors\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST(\n",
    "        \"./\",\n",
    "        download=True,\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "    \n",
    "    test_dataset = datasets.MNIST(\n",
    "        \"./\",\n",
    "        download=True,\n",
    "        train=False,\n",
    "        transform=transform,\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    def init_weights(shape):\n",
    "        # Kaiming He initialization (a good initialization is important)\n",
    "        # https://arxiv.org/abs/1502.01852\n",
    "        std = np.sqrt(2. / shape[0])\n",
    "        w = torch.randn(size=shape) * std\n",
    "        w.requires_grad = True\n",
    "        return w\n",
    "    \n",
    "    def init_PRelu_weights(shape):\n",
    "        \n",
    "        #Initialize with a as being \"normal\" ReLU\n",
    "        a = torch.zeros(shape)\n",
    "        \n",
    "        #Make sure gradients are computed \n",
    "        a.requires_grad = True\n",
    "        return a \n",
    "    \n",
    "    \n",
    "    def rectify(x):\n",
    "        # Rectified Linear Unit (ReLU)\n",
    "        return torch.max(torch.zeros_like(x), x)\n",
    "    \n",
    "    def PRelu(X, a):\n",
    "        # Parametric ReLU Implementation\n",
    "        return torch.where(X > 0, X, a*X)\n",
    "    \n",
    "    \n",
    "    class RMSprop(optim.Optimizer):\n",
    "        \"\"\"\n",
    "        This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "        It serves here as an example\n",
    "        \"\"\"\n",
    "        def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "            defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "            super(RMSprop, self).__init__(params, defaults)\n",
    "    \n",
    "        def step(self):\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    grad = p.grad.data\n",
    "                    state = self.state[p]\n",
    "    \n",
    "                    # state initialization\n",
    "                    if len(state) == 0:\n",
    "                        state['square_avg'] = torch.zeros_like(p.data)\n",
    "    \n",
    "                    square_avg = state['square_avg']\n",
    "                    alpha = group['alpha']\n",
    "    \n",
    "                    # update running averages\n",
    "                    square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                    avg = square_avg.sqrt().add_(group['eps'])\n",
    "    \n",
    "                    # gradient update\n",
    "                    p.data.addcdiv_(grad, avg, value=-group['lr'])\n",
    "    \n",
    "    \n",
    "    # define the neural network\n",
    "    def model(x, w_h, w_h2, w_o):\n",
    "        h = rectify(x @ w_h)\n",
    "        h2 = rectify(h @ w_h2)\n",
    "        pre_softmax = h2 @ w_o\n",
    "        return pre_softmax\n",
    "    \n",
    "    #Define dropout method\n",
    "    def dropout(X, p_drop = 0.5):\n",
    "        if (0 < p_drop < 1):\n",
    "            return torch.where(torch.from_numpy(np.random.binomial(1, 0.5, size = X.shape)) == 1, 0, X/(1-p_drop))\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    # define the neural network with dropout\n",
    "    def dropout_model(x, w_h, w_h2, w_o, p_drop_out, p_drop_hidden):\n",
    "        h = rectify(dropout(x, p_drop_out) @ w_h)\n",
    "        h2 = rectify(dropout(h, p_drop_hidden) @ w_h2)\n",
    "        pre_softmax = dropout(h2, p_drop_hidden) @ w_o\n",
    "        return pre_softmax\n",
    "    \n",
    "    # define the neural network with dropout AND PReLU\n",
    "    def dropout_model_PRelu(x, w_h, w_h2, w_o, a, p_drop_out, p_drop_hidden):\n",
    "        h = PRelu(dropout(x, p_drop_out) @ w_h, a)\n",
    "        h2 = PRelu(dropout(h, p_drop_hidden) @ w_h2, a)\n",
    "        pre_softmax = dropout(h2, p_drop_hidden) @ w_o\n",
    "        return pre_softmax\n",
    "    \n",
    "    # define the neural network with PReLU instead of normal ReLU\n",
    "    def model_PRelu(x, w_h, w_h2, w_o, a):\n",
    "        h = PRelu(x @ w_h, a)\n",
    "        h2 = PRelu(h @ w_h2, a)\n",
    "        pre_softmax = h2 @ w_o\n",
    "        return pre_softmax\n",
    "    \n",
    "    #Loop over different models\n",
    "    models = [model, dropout_model, model_PRelu, dropout_model_PRelu]\n",
    "    name_models = ['ReLU', 'ReLU with dropout', 'PReLU', 'PReLU with dropout']\n",
    "    train_loss_models = []\n",
    "    test_loss_models = []\n",
    "    \n",
    "    i = 0\n",
    "    for model_chosen in models:\n",
    "        \n",
    "        # initialize weights and a of PRelu\n",
    "        \n",
    "        # input shape is (B, 784)\n",
    "        w_h = init_weights((784, 625))\n",
    "        # hidden layer with 625 neurons\n",
    "        w_h2 = init_weights((625, 625))\n",
    "        # hidden layer with 625 neurons\n",
    "        w_o = init_weights((625, 10))\n",
    "        # output shape is (B, 10)\n",
    "        \n",
    "        #a vector initialization\n",
    "        a = init_PRelu_weights(625)\n",
    "        \n",
    "        \n",
    "        if model_chosen == model or model_chosen == dropout_model:\n",
    "            optimizer = RMSprop(params=[w_h, w_h2, w_o])\n",
    "        else:\n",
    "            optimizer = RMSprop(params=[w_h, w_h2, w_o, a])\n",
    "        \n",
    "        n_epochs = 20\n",
    "        \n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        \n",
    "        # put this into a training loop over 100 epochs\n",
    "        for epoch in tqdm(range(n_epochs + 1)):\n",
    "            train_loss_this_epoch = []\n",
    "            for idx, batch in enumerate(train_dataloader):\n",
    "                x, y = batch\n",
    "        \n",
    "                # our model requires flattened input\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                # feed input through model\n",
    "                if model_chosen == model:\n",
    "                    noise_py_x = model_chosen(x, w_h, w_h2, w_o)\n",
    "                elif model_chosen == dropout_model:\n",
    "                    noise_py_x = model_chosen(x, w_h, w_h2, w_o, p_drop_out = 0.5, p_drop_hidden = 0.5)\n",
    "                elif model_chosen == model_PRelu:\n",
    "                    noise_py_x = model_chosen(x, w_h, w_h2, w_o, a)\n",
    "                else: \n",
    "                    noise_py_x = model_chosen(x, w_h, w_h2, w_o, a, p_drop_out = 0.5, p_drop_hidden = 0.5)\n",
    "            \n",
    "        \n",
    "                # reset the gradient\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "                # the cross-entropy loss function already contains the softmax\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "        \n",
    "                train_loss_this_epoch.append(float(loss))\n",
    "        \n",
    "                # compute the gradient\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "        \n",
    "            train_loss.append(np.mean(train_loss_this_epoch))\n",
    "        \n",
    "            # test periodically\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}\")\n",
    "                print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "                test_loss_this_epoch = []\n",
    "        \n",
    "                # no need to compute gradients for validation\n",
    "                with torch.no_grad():\n",
    "                    for idx, batch in enumerate(test_dataloader):\n",
    "                        x, y = batch\n",
    "                        x = x.reshape(batch_size, 784)\n",
    "                        \n",
    "                        if model_chosen == model:\n",
    "                            noise_py_x = model_chosen(x, w_h, w_h2, w_o)\n",
    "                        elif model_chosen == dropout_model:\n",
    "                            noise_py_x = model_chosen(x, w_h, w_h2, w_o, p_drop_out = 0.5, p_drop_hidden = 0.5)\n",
    "                        elif model_chosen == model_PRelu:\n",
    "                            noise_py_x = model_chosen(x, w_h, w_h2, w_o, a)\n",
    "                        else: \n",
    "                            noise_py_x = model_chosen(x, w_h, w_h2, w_o, a, p_drop_out = 0.5, p_drop_hidden = 0.5)\n",
    "                            \n",
    "                        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                        test_loss_this_epoch.append(float(loss))\n",
    "        \n",
    "                test_loss.append(np.mean(test_loss_this_epoch))\n",
    "                \n",
    "                print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "        \n",
    "        \n",
    "        train_loss_models.append(train_loss)\n",
    "        test_loss_models.append(test_loss)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "        plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "        plt.title(\"Train and Test Loss over Training: \" + name_models[i])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        i += 1\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c206b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
